{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7864a5f1",
   "metadata": {},
   "source": [
    "## Resume Parser "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8392ae44",
   "metadata": {},
   "source": [
    "### 1. Data Gathering and Processing\n",
    "\n",
    "To create a diverse and representative dataset for this project, I gathered a small set of **7 synthetic resumes** in both PDF and Word formats. This approach ensures the solution is not overfitted to any specific resume style and can handle variations in document structure. This small sample will also be used for evaluating the project's performance.\n",
    "\n",
    "The resume samples were sourced from the following platforms:\n",
    "\n",
    "* **Indeed Resume Builder** ([www.resume.com](https://www.resume.com)): I created and downloaded **3 resumes in PDF format** using this free tool. The profile information (names, emails, and skills) was generated using an AI tool (Gemini 2.5 Flash) to ensure the data was fictitious and unique to this project.\n",
    "* **Microsoft 365 Resume Templates** ([create.microsoft.com/en-us/templates/resumes](https://create.microsoft.com/en-us/templates/resumes)): To obtain samples in the **Word (.docx) format**, I utilized the free resume template bank provided by Microsoft. I created and downloaded **4 resumes in .docx format**. These templates cover a range of professional layouts and designs, which helps in testing the robustness of the parsing algorithm across different document structures.\n",
    "\n",
    "By using these methods, I was able to create a small but effective dataset to develop and evaluate the resume parser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce99721",
   "metadata": {},
   "source": [
    "### 2. Proof of Concept (POC) and System Design\n",
    "\n",
    "The development of this resume parser will follow a **Test-Driven Development (TDD)** approach. This means we will start by writing tests that specify the expected behavior of our solution. These tests will serve as our development roadmap, ensuring that our code correctly handles the required inputs and produces the desired structured output.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2.1. Overall System Pipeline\n",
    "\n",
    "The core of this project is a robust pipeline designed to extract structured information from various resume file formats. The proposed system design is a multi-step process that leverages specialized libraries for document conversion and the power of a large language model (LLM) for intelligent information extraction.\n",
    "\n",
    "The high-level pipeline can be summarized as follows:\n",
    "\n",
    "1.  **TDD Driver**: The development begins with test cases written in `pytest`, which define the expected `name`, `email`, and `skills` for our sample resumes.\n",
    "2.  **Document Ingestion**: The system accepts resumes in either PDF or Word (.docx) format.\n",
    "3.  **Document-to-Markdown Conversion**: Both file types are converted into a standardized Markdown format. This is a critical step, as Markdown preserves key structural elements (like headings, lists, and bold text) while creating a clean, text-based representation. This format is also ideal for processing by LLMs, as they are often pre-trained on a vast corpus of Markdown text, leading to more predictable and accurate results.\n",
    "4.  **LLM-based Information Extraction**: The Markdown text of the resume is passed to a powerful LLM with a carefully crafted prompt. The prompt directs the LLM to identify and extract the candidate's name, email, and skills.\n",
    "5.  **Structured Output Generation**: The LLM is configured to return the extracted information in a structured JSON format, ensuring a clean and consistent output.\n",
    "6.  **Output Validation**: The final JSON object is validated against a defined schema, and the test cases verify that the output matches the ground truth.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2.2. Technical Stack and Tooling\n",
    "\n",
    "To implement this pipeline, I propose using a specific set of tools and packages for each step, balancing efficiency, cost, and maintainability.\n",
    "\n",
    "**Testing:**\n",
    "\n",
    "* The project will be **test-driven using `pytest`**. Tests will be written to fail initially, guiding the implementation of the parsing logic until they pass. This ensures all functionality is verified and provides a safety net for future refactoring.\n",
    "\n",
    "**Document Conversion:**\n",
    "\n",
    "* **Word (.docx) to Markdown**: The `markdown-it-doc` package is the primary choice for its reliability in converting Microsoft Word documents to Markdown. As a fallback, `pandoc` can be used, a versatile command-line tool known for its extensive support for various document conversions.\n",
    "* **PDF to Markdown**: For PDF parsing, `pymupdf4llm` will be leveraged. This library is specifically designed to handle the complexities of PDF layouts and produce LLM-friendly output. It provides a more robust solution compared to general-purpose PDF-to-text libraries. A simple fallback would be `PyMuPDF`, which offers basic text extraction capabilities.\n",
    "\n",
    "**LLM and Prompt Engineering:**\n",
    "\n",
    "* **LLM Service**: The API from [OpenRouter](https://openrouter.ai/) will be used to access various LLMs, prioritizing **`Gpt4-mini`** due to its low cost and strong performance. This choice balances accuracy with cost-effectiveness.\n",
    "* **Structured Output**: A key challenge with the OpenRouter API is the lack of explicit, well-documented support for structured output formats (like a JSON schema). To address this, a robust method will be implemented using a combination of a well-designed prompt and the Python `pydantic` library.\n",
    "    * **Prompting**: The LLM will be given clear instructions to return the output in a specific JSON format, explicitly listing the keys (`name`, `email`, `skills`) and their corresponding data types.\n",
    "    * **Pydantic**: The `pydantic` library will be used to define a data model that mirrors the required JSON schema. The LLM's output will be passed to this model, which will handle the parsing and validation. This approach ensures the final output is always in the correct format, handles potential parsing errors gracefully, and provides clear type-hinting for clean code.\n",
    "\n",
    "**Project Management and Development Practices:**\n",
    "\n",
    "* **Environment Management**: `uv` will be used for project and dependency management.\n",
    "* **Version Control**: Git will be used for source control, with the project hosted on a private GitHub repository for development, which will be made public upon completion to satisfy the project requirements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82cd212",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99133675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from markitdown import MarkItDown\n",
    "import dotenv\n",
    "import requests\n",
    "import json\n",
    "from typing import Type, List, Optional, Dict, Any\n",
    "from pydantic import BaseModel, Field\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2f5c2e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load openrouter api key\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f499a8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_resume_parser():\n",
    "    \"\"\"Simple TDD test function with multiple test cases\"\"\"\n",
    "    parser = ResumeParser()\n",
    "    \n",
    "    # Test case 1: Classic management resume (DOCX)\n",
    "    print(\"Testing Classic management resume...\")\n",
    "    result1 = parser(\"./data/inputs/Classic management resume.docx\")\n",
    "    expected1 = {\n",
    "        \"name\": \"Carmelo Barese\",\n",
    "        \"email\": \"carmelo@example.com\",\n",
    "        \"skills\": [\"Marketing\", \"Communication\", \"Project management\", \"Problem-solving\", \"Budget planning\"]\n",
    "    }\n",
    "    \n",
    "    assert result1[\"name\"] == expected1[\"name\"], f\"Test 1 - Name: Expected {expected1['name']}, got {result1['name']}\"\n",
    "    assert result1[\"email\"] == expected1[\"email\"], f\"Test 1 - Email: Expected {expected1['email']}, got {result1['email']}\"\n",
    "    assert set(result1[\"skills\"]) == set(expected1[\"skills\"]), f\"Test 1 - Skills mismatch\"\n",
    "    print(\"âœ… Test 1 passed!\")\n",
    "    \n",
    "    # Test case 2: Customer Production Assistant resume (PDF)\n",
    "    print(\"Testing Customer Production Assistant resume...\")\n",
    "    result2 = parser(\"data/inputs/Customer Production Assistant Resume.pdf\")\n",
    "    expected2 = {\n",
    "        \"name\": \"Jessica Garcia\",\n",
    "        \"email\": \"jessgarcia@gmail.com\",\n",
    "        \"skills\": [\"Microsoft Office\", \"Basic Math\", \"Communication Skills\", \"Microsoft Outlook\", \"Manufacturing\", \"Computer Skills\", \"Microsoft Excel\", \"Education\"]\n",
    "    }\n",
    "    \n",
    "    assert result2[\"name\"] == expected2[\"name\"], f\"Test 2 - Name: Expected {expected2['name']}, got {result2['name']}\"\n",
    "    assert result2[\"email\"] == expected2[\"email\"], f\"Test 2 - Email: Expected {expected2['email']}, got {result2['email']}\"\n",
    "    assert set(result2[\"skills\"]) == set(expected2[\"skills\"]), f\"Test 2 - Skills mismatch\"\n",
    "    print(\"âœ… Test 2 passed!\")\n",
    "    \n",
    "    print(\"ðŸŽ‰ All tests passed! Both DOCX and PDF parsing work correctly.\")\n",
    "\n",
    "class ResumeProfile(BaseModel):\n",
    "    \"\"\"Structured Resume Profile informations.\"\"\"\n",
    "    name: str = Field(..., description=\"Full name of the candidate\")\n",
    "    email: str = Field(..., description=\"email address\")\n",
    "    skills: List[str] = Field(..., description=\"List of skills\")\n",
    "\n",
    "class ResumeParser:\n",
    "    DEFAULT_TASK_PROMPT = \"\"\"\n",
    "        You are an expert at extracting structured data from resumes.\n",
    "        Extract the full name, email and a list of skills from the provided resume content and return the data as a JSON object that matches specified schema.\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self, task_prompt: str = DEFAULT_TASK_PROMPT):\n",
    "        self.task_prompt = task_prompt\n",
    "        self.md_converter = MarkItDown(enable_plugins=False)\n",
    "        \n",
    "    def convert_to_md(self, filepath:str) -> str:\n",
    "        if not filepath.endswith(\".docx\"):\n",
    "            raise ValueError(\"Unsupported file format. Only .docx is supported.\")\n",
    "        else:\n",
    "            result =  self.md_converter.convert(filepath)\n",
    "            return result.text_content\n",
    "\n",
    "    def extract_resume_data_llm(\n",
    "        self,\n",
    "        resume_content: str,\n",
    "        output_model: Type[BaseModel] = ResumeProfile,\n",
    "        model: str = \"openai/gpt-4.1-mini\"\n",
    "    ) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Call LLM API to extract structured information from resume content.\n",
    "        \n",
    "        Args:\n",
    "            resume_content: Raw text content (markdown) of the resume to analyze\n",
    "            task_prompt: Custom system prompt for the LLM\n",
    "            output_model: Pydantic model class for response validation\n",
    "            model: LLM model identifier to use\n",
    "        \n",
    "        Returns:\n",
    "            Optional[Dict[str, Any]]: Structured resume data if successful, None on failure\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If API response is invalid\n",
    "            requests.RequestException: For API communication errors\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Create model instance for schema validation\n",
    "            generated_schema = output_model.model_json_schema()\n",
    "            api_schema_payload = {\n",
    "                \"name\": \"ResumeProfile\",\n",
    "                \"strict\": True,\n",
    "                \"schema\": {\n",
    "                    **generated_schema,\n",
    "                    \"additionalProperties\": False \n",
    "                }\n",
    "            }\n",
    "            \n",
    "            response = requests.post(\n",
    "                \"https://openrouter.ai/api/v1/chat/completions\",\n",
    "                headers={\n",
    "                    \"Authorization\": f\"Bearer {os.getenv('API_KEY')}\",\n",
    "                    \"Content-Type\": \"application/json\",\n",
    "                },\n",
    "                json={\n",
    "                    \"model\": model,\n",
    "                    \"messages\": [\n",
    "                        {\"role\": \"system\", \"content\": self.task_prompt},\n",
    "                        {\"role\": \"user\", \"content\": resume_content},\n",
    "                    ],\n",
    "                    \"response_format\": {\n",
    "                        \"type\": \"json_schema\",\n",
    "                        \"json_schema\": api_schema_payload\n",
    "                    },\n",
    "                },\n",
    "            )\n",
    "\n",
    "            data = response.json()\n",
    "            if \"choices\" not in data or not data[\"choices\"]:\n",
    "                raise ValueError(\"Invalid API response format\")\n",
    "                \n",
    "            content = data[\"choices\"][0][\"message\"][\"content\"]\n",
    "            \n",
    "            # Validate and convert to dict\n",
    "            parsed_data = output_model.model_validate_json(content)\n",
    "            return parsed_data.model_dump()\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"API request failed: {str(e)}\")\n",
    "            return None\n",
    "        except ValueError as e:\n",
    "            print(f\"Data validation failed: {str(e)}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def __call__(self, filepath):\n",
    "        # Implementation will go here - starts with failing test\n",
    "        md_content = self.convert_to_md(filepath)\n",
    "        res = self.extract_resume_data_llm(resume_content=md_content)\n",
    "        return res\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "187a732d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Classic management resume...\n",
      "âœ… Test 1 passed!\n",
      "Testing Customer Production Assistant resume...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unsupported file format. Only .docx is supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtest_resume_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mtest_resume_parser\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Test case 2: Customer Production Assistant resume (PDF)\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTesting Customer Production Assistant resume...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m result2 = \u001b[43mparser\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata/inputs/Customer Production Assistant Resume.pdf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m expected2 = {\n\u001b[32m     23\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mJessica Garcia\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     24\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33memail\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mjessgarcia@gmail.com\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     25\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mskills\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mMicrosoft Office\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mBasic Math\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mCommunication Skills\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mMicrosoft Outlook\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mManufacturing\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mComputer Skills\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mMicrosoft Excel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mEducation\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     26\u001b[39m }\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m result2[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m] == expected2[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTest 2 - Name: Expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected2[\u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult2[\u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 132\u001b[39m, in \u001b[36mResumeParser.__call__\u001b[39m\u001b[34m(self, filepath)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, filepath):\n\u001b[32m    131\u001b[39m     \u001b[38;5;66;03m# Implementation will go here - starts with failing test\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m     md_content = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_to_md\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m     res = \u001b[38;5;28mself\u001b[39m.extract_resume_data_llm(resume_content=md_content)\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 53\u001b[39m, in \u001b[36mResumeParser.convert_to_md\u001b[39m\u001b[34m(self, filepath)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconvert_to_md\u001b[39m(\u001b[38;5;28mself\u001b[39m, filepath:\u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m     52\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filepath.endswith(\u001b[33m\"\u001b[39m\u001b[33m.docx\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mUnsupported file format. Only .docx is supported.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     55\u001b[39m         result =  \u001b[38;5;28mself\u001b[39m.md_converter.convert(filepath)\n",
      "\u001b[31mValueError\u001b[39m: Unsupported file format. Only .docx is supported."
     ]
    }
   ],
   "source": [
    "test_resume_parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e16b0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Carmelo Barese  Assistant Manager | | | | | | |\n",
      "| --- | --- | --- | --- | --- | --- | --- |\n",
      "|  | |  | | |  | |\n",
      "| 312.555.0110 | | carmelo@example.com | | | Seattle, WA | |\n",
      "|  | |  | | |  | |\n",
      "| Objective | As an assistant manager, my primary objective is to support the functioning of business operations by managing staff, coordinating operations, and ensuring exceptional customer service. I aim to create a positive and productive work environment by communicating with team members, setting clear goals, and monitoring performance. | | | | |\n",
      "| Education | Mount Flores College New York City, NY  BA in Business Administration  GPA 3.87 | | | | |\n",
      "| Key Skills | Marketing  Project management  Budget planning | | Communication  Problem-solving | | |\n",
      "| Experience | Responsibilities: overseeing daily operations, managing staff, ensuring compliance with banking regulations, and providing exceptional customer service. Also responsible for analyzing financial data, identifying trends, and developing strategies to improve the bank's performance. | | | | |\n",
      "|  | | | | | |\n",
      "| June 20XX - Present Assistant Manager  Woodgrove Bank | | Jan 20XX â€“ June 20XX Lead Salesperson  Safewest Banking | | Aug 20XX â€“ Jan 20XX Sales Associate  Safewest Banking | |\n",
      "| Communication | Implemented new procedures and technologies that improved efficiency and streamlined operations. | | | | |\n",
      "| Leadership | Successfully led a team to exceed sales goals while maintaining excellent customer satisfaction scores. | | | | |\n",
      "| References | Available upon request. | | | | |\n"
     ]
    }
   ],
   "source": [
    "\n",
    "md = MarkItDown(enable_plugins=False) # Set to True to enable plugins\n",
    "result = md.convert(\"data/inputs/Classic management resume.docx\")\n",
    "print(result.text_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b1d10919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from typing import Type, List, Optional, Dict, Any\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "task_prompt = \"\"\"\n",
    "    You are an expert at extracting structured data from resumes. \n",
    "    Extract the full name, email and a list of skills from the provided resume content and return teh data as JSON object that matches specified schema.\n",
    "    \"\"\"\n",
    "\n",
    "class ResumeProfile(BaseModel):\n",
    "    \"\"\"Structured Resume Profile informations.\"\"\"\n",
    "    name: str = Field(..., description=\"Full name of the candidate\")\n",
    "    email: str = Field(..., description=\"email address\")\n",
    "    skills: List[str] = Field(..., description=\"List of skills\")\n",
    "\n",
    "\n",
    "def extract_resume_data_llm(\n",
    "    resume_content: str,\n",
    "    task_prompt: str = task_prompt,\n",
    "    output_model: Type[BaseModel] = ResumeProfile,\n",
    "    model: str = \"openai/gpt-4.1-mini\"\n",
    ") -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Call LLM API to extract structured information from resume content.\n",
    "    \n",
    "    Args:\n",
    "        resume_content: Raw text content (markdown) of the resume to analyze\n",
    "        task_prompt: Custom system prompt for the LLM\n",
    "        output_model: Pydantic model class for response validation\n",
    "        model: LLM model identifier to use\n",
    "    \n",
    "    Returns:\n",
    "        Optional[Dict[str, Any]]: Structured resume data if successful, None on failure\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If API response is invalid\n",
    "        requests.RequestException: For API communication errors\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create model instance for schema validation\n",
    "        generated_schema = output_model.model_json_schema()\n",
    "        api_schema_payload = {\n",
    "            \"name\": \"ResumeProfile\",\n",
    "            \"strict\": True,\n",
    "            \"schema\": {\n",
    "                **generated_schema,\n",
    "                \"additionalProperties\": False \n",
    "            }\n",
    "        }\n",
    "        \n",
    "        response = requests.post(\n",
    "            \"https://openrouter.ai/api/v1/chat/completions\",\n",
    "            headers={\n",
    "                \"Authorization\": f\"Bearer {os.getenv('API_KEY')}\",\n",
    "                \"Content-Type\": \"application/json\",\n",
    "            },\n",
    "            json={\n",
    "                \"model\": model,\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": task_prompt},\n",
    "                    {\"role\": \"user\", \"content\": resume_content},\n",
    "                ],\n",
    "                \"response_format\": {\n",
    "                    \"type\": \"json_schema\",\n",
    "                    \"json_schema\": api_schema_payload\n",
    "                },\n",
    "            },\n",
    "        )\n",
    "\n",
    "        data = response.json()\n",
    "        if \"choices\" not in data or not data[\"choices\"]:\n",
    "            raise ValueError(\"Invalid API response format\")\n",
    "            \n",
    "        content = data[\"choices\"][0][\"message\"][\"content\"]\n",
    "        \n",
    "        # Validate and convert to dict\n",
    "        parsed_data = output_model.model_validate_json(content)\n",
    "        return parsed_data.model_dump()\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"API request failed: {str(e)}\")\n",
    "        return None\n",
    "    except ValueError as e:\n",
    "        print(f\"Data validation failed: {str(e)}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {str(e)}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1ad2d805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Carmelo Barese',\n",
       " 'email': 'carmelo@example.com',\n",
       " 'skills': ['Marketing',\n",
       "  'Project management',\n",
       "  'Budget planning',\n",
       "  'Communication',\n",
       "  'Problem-solving']}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test \n",
    "resume_test = result.text_content\n",
    "res = extract_resume_data_llm(resume_test)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "30dd1089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e83de9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
